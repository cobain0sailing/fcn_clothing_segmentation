{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_kernel_size(factor):\n",
    "    \"\"\"\n",
    "    给定所需的上采样因子，确定卷积核的大小\n",
    "    \"\"\"\n",
    "    return 2 * factor - factor % 2\n",
    "\n",
    "\n",
    "def upsample_filt(size):\n",
    "    \"\"\"\n",
    "    创建一个给定(h, w) 大小的适用于上采样过程的二维双线性卷积核\n",
    "    \"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factor, number_of_classes):\n",
    "    \"\"\"\n",
    "    使用双线性卷积核，为转置卷积创建权重矩阵\n",
    "    初始化\n",
    "    \"\"\"\n",
    "\n",
    "    filter_size = get_kernel_size(factor)\n",
    "\n",
    "    weights = np.zeros((filter_size,\n",
    "                        filter_size,\n",
    "                        number_of_classes,\n",
    "                        number_of_classes), dtype=np.float32)\n",
    "\n",
    "    upsample_kernel = upsample_filt(filter_size)\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "\n",
    "        weights[:, :, i, i] = upsample_kernel\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "sys.path.append(\"D:\\\\Git\\\\dpakhom1\\\\workspace\\\\models\\\\slim\")\n",
    "sys.path.insert(0,'D:\\\\Git\\\\dpakhom1\\\\workspace\\\\models\\\\slim')\n",
    "checkpoints_dir = 'D:\\\\Git\\\\dpakhom1\\\\checkpoints'\n",
    "\n",
    "image_filename = \"cat.jpg\"\n",
    "annotation_filename = \"cat_annotation.png\"\n",
    "\n",
    "image_filename_placeholder = tf.placeholder(tf.string)\n",
    "annotation_filename_placeholder = tf.placeholder(tf.string)\n",
    "is_training_placeholder = tf.placeholder(tf.bool)\n",
    "\n",
    "feed_dict_to_use = {image_filename_placeholder: image_filename,\n",
    "                    annotation_filename_placeholder: annotation_filename,\n",
    "                    is_training_placeholder: True}\n",
    "\n",
    "image_tensor = tf.read_file(image_filename_placeholder)\n",
    "annotation_tensor = tf.read_file(annotation_filename_placeholder)\n",
    "\n",
    "image_tensor = tf.image.decode_jpeg(image_tensor, channels=3)\n",
    "annotation_tensor = tf.image.decode_png(annotation_tensor, channels=1)\n",
    "\n",
    "# 对于每个类别，将其设置为1而不是一个数字——我们在后续计算\n",
    "# 交叉熵时会用到。有的时候，实际分割区域的掩码可以有很多值，\n",
    "# 不仅仅为0和1\n",
    "\n",
    "class_labels_tensor = tf.equal(annotation_tensor, 1)\n",
    "background_labels_tensor = tf.not_equal(annotation_tensor, 1)\n",
    "\n",
    "#将布尔值转换为浮点数——这样才能正确地计算交叉熵损失\n",
    "bit_mask_class = tf.to_float(class_labels_tensor)\n",
    "bit_mask_background = tf.to_float(background_labels_tensor)\n",
    "\n",
    "combined_mask = tf.concat(axis=2, values=[bit_mask_class,\n",
    "                                                bit_mask_background])\n",
    "\n",
    "# 调整输入数据的大小，使其与tf.softmax_cross_entropy_with_logits中\n",
    "# [batch_size, num_classes]的要求保持一致\n",
    "flat_labels = tf.reshape(combined_mask,[-1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable adam_vars/vgg_16/conv1/conv1_1/weights/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-747247891e0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_to_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;31m# 在这里，我们定义了一个函数，调用时会从VGG模型检查点中读取权重数据，并加载至变量中。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    410\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;31m# Create slots for the first and second moments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[1;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m       \u001b[0mnamed_slots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[1;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[0;32m    121\u001b[0m   \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m   return create_slot(primary, val, name,\n\u001b[1;32m--> 123\u001b[1;33m                      colocate_with_primary=colocate_with_primary)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot\u001b[1;34m(primary, val, name, colocate_with_primary)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolocate_with_primary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[1;34m(primary, val, scope)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[0mcurrent_partitioner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m   \u001b[0mslot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m       custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m           custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m           validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    637\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 639\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable adam_vars/vgg_16/conv1/conv1_1/weights/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig_size = [15, 4]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from nets import vgg\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "# 加载像素均值以及为每个像素进行减法运算的函数\n",
    "from preprocessing.vgg_preprocessing import (_mean_image_subtraction,\n",
    "                                            _R_MEAN, _G_MEAN, _B_MEAN)\n",
    "\n",
    "upsample_factor = 32\n",
    "number_of_classes = 2\n",
    "log_folder = './logs/1'\n",
    "\n",
    "vgg_checkpoint_path = os.path.join(checkpoints_dir, 'vgg_16.ckpt')\n",
    "\n",
    "# 在与像素均值做差前，将图像转换至float32类型\n",
    "image_float = tf.to_float(image_tensor, name='ToFloat')\n",
    "\n",
    "# 将每个像素的具体数值与像素均值做差\n",
    "mean_centered_image = _mean_image_subtraction(image_float,\n",
    "                                          [_R_MEAN, _G_MEAN, _B_MEAN])\n",
    "\n",
    "processed_images = tf.expand_dims(mean_centered_image, 0)\n",
    "\n",
    "upsample_filter_np = bilinear_upsample_weights(upsample_factor,\n",
    "                                               number_of_classes)\n",
    "\n",
    "upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "\n",
    "# 定义将要使用的模型——指定在最后一层仅使用两个类别\n",
    "with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "\n",
    "    logits, end_points = vgg.vgg_16(processed_images,\n",
    "                           num_classes=2,\n",
    "                           is_training=is_training_placeholder,\n",
    "                           spatial_squeeze=False)\n",
    "                           #fc_conv_padding='SAME')\n",
    "\n",
    "downsampled_logits_shape = tf.shape(logits)\n",
    "\n",
    "# 计算上采样数据的输出大小\n",
    "upsampled_logits_shape = tf.stack([\n",
    "                                  downsampled_logits_shape[0],\n",
    "                                  downsampled_logits_shape[1] * upsample_factor,\n",
    "                                  downsampled_logits_shape[2] * upsample_factor,\n",
    "                                  downsampled_logits_shape[3]\n",
    "                                 ])\n",
    "\n",
    "# 进行上采样处理\n",
    "upsampled_logits = tf.nn.conv2d_transpose(logits, upsample_filter_tensor,\n",
    "                                 output_shape=upsampled_logits_shape,\n",
    "                                 strides=[1, upsample_factor, upsample_factor, 1])\n",
    "\n",
    "# 展开预测结果，以便于我们计算每个像素的交叉熵，并获得交叉熵的总和\n",
    "flat_logits = tf.reshape(upsampled_logits, [-1, number_of_classes])\n",
    "\n",
    "cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=flat_logits,\n",
    "                                                          labels=flat_labels)\n",
    "\n",
    "cross_entropy_sum = tf.reduce_sum(cross_entropies)\n",
    "\n",
    "# 获得每个像素的最终预测结果——请注意，在这种情况下我们并不需要\n",
    "# 使用softmax，因为我们只需要得到最终的决策。如果我们还需要各\n",
    "# 个类别的概率，那么我们必须应用softmax\n",
    "\n",
    "pred = tf.argmax(upsampled_logits, axis=3)\n",
    "\n",
    "probabilities = tf.nn.softmax(upsampled_logits)\n",
    "\n",
    "# 在这里我们定义了一个优化器，并添加了所有将要创建至命名\n",
    "# 空间'adam_vars'下的变量。这样做有利于我们后续轻松地访问\n",
    "# 它们。这些变量供adam优化器使用，并且与vgg模型中的变量无关\n",
    "\n",
    "# 我们还获得了每个变量的梯度数据\n",
    "# 这样，我们可以在tensorboard中可视化这些变量\n",
    "# optimizer.compute_gradients与optimizer.apply_gradients\n",
    "# 等价于执行：\n",
    "# train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy_sum)\n",
    "with tf.variable_scope(\"adam_vars\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "    gradients = optimizer.compute_gradients(loss=cross_entropy_sum)\n",
    "\n",
    "    for grad_var_pair in gradients:\n",
    "\n",
    "        current_variable = grad_var_pair[1]\n",
    "        current_gradient = grad_var_pair[0]\n",
    "\n",
    "        # 替换原始变量名中的一些字符\n",
    "        # tensorboard不支持':'符号\n",
    "        gradient_name_to_save = current_variable.name.replace(\":\", \"_\")\n",
    "\n",
    "        # 得到每一层的梯度直方图，并随后在tensorboard中可视化这些数据\n",
    "        # tensorboard\n",
    "        tf.summary.histogram(gradient_name_to_save, current_gradient) \n",
    "\n",
    "    train_step = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# 在这里，我们定义了一个函数，调用时会从VGG模型检查点中读取权重数据，并加载至变量中。\n",
    "# 我们从负责类别预测的最后一层中剔除了权重。我们这样做是因为我们将有不同数量的\n",
    "# 类别进行预测，我们不能在初始化时使用原先的类别。\n",
    "vgg_except_fc8_weights = slim.get_variables_to_restore(exclude=['vgg_16/fc8', 'adam_vars'])\n",
    "\n",
    "# 这里我们得到了网络中最后一层的权重变量\n",
    "# 正如我们看到的，VGG最初训练的类别数量与我们实际的类别数量\n",
    "# 并不相同——在我们的情况下，总共只有两类\n",
    "vgg_fc8_weights = slim.get_variables_to_restore(include=['vgg_16/fc8'])\n",
    "\n",
    "adam_optimizer_variables = slim.get_variables_to_restore(include=['adam_vars'])\n",
    "# 为模型损失添加一个summary OP——以便我们可以在tensorboard中看到它\n",
    "tf.summary.scalar('cross_entropy_loss', cross_entropy_sum)\n",
    "\n",
    "# 将所有summary OP合并至一个OP总\n",
    "# 在运行程序时生成字符串\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# 创建一个summary writer——用于将所有日志写入到一个特定文件中\n",
    "# 这个文件后续可以由tensorboard读取\n",
    "summary_string_writer = tf.summary.FileWriter(log_folder)\n",
    "\n",
    "# 如果日志文件夹尚未存在，则创建一个新的文件夹\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "# 创建一个OP，对VGG模型中各权重变量进行初始化操作\n",
    "read_vgg_weights_except_fc8_func = slim.assign_from_checkpoint_fn(\n",
    "                                   vgg_checkpoint_path,\n",
    "                                   vgg_except_fc8_weights)\n",
    "\n",
    "# 针对新的fc8层权重数据的初始化器——仅包括两类\n",
    "vgg_fc8_weights_initializer = tf.variables_initializer(vgg_fc8_weights)\n",
    "\n",
    "# adam变量的初始化器\n",
    "optimization_variables_initializer = tf.variables_initializer(adam_optimizer_variables)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 运行初始化器\n",
    "    read_vgg_weights_except_fc8_func(sess)\n",
    "    sess.run(vgg_fc8_weights_initializer)\n",
    "    sess.run(optimization_variables_initializer)\n",
    "\n",
    "    train_image, train_annotation = sess.run([image_tensor, annotation_tensor],\n",
    "                                              feed_dict=feed_dict_to_use)\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    ax1.imshow(train_image)\n",
    "    ax1.set_title('Input image')\n",
    "    probability_graph = ax2.imshow(np.dstack((train_annotation,)*3)*100)\n",
    "    ax2.set_title('Input Ground-Truth Annotation')\n",
    "    plt.show()\n",
    "\n",
    "    # 执行10次迭代\n",
    "    for i in range(10):\n",
    "\n",
    "        #loss, summary_string = sess.run([cross_entropy_sum, merged_summary_op],\n",
    "        #                                feed_dict=feed_dict_to_use)\n",
    "\n",
    "        sess.run(train_step, feed_dict=feed_dict_to_use)\n",
    "\n",
    "        pred_np, probabilities_np = sess.run([pred, probabilities],\n",
    "                                              feed_dict=feed_dict_to_use)\n",
    "\n",
    "        #summary_string_writer.add_summary(summary_string, i)\n",
    "\n",
    "        cmap = plt.get_cmap('bwr')\n",
    "\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "        ax1.imshow(np.uint8(pred_np.squeeze() != 1), vmax=1.5, vmin=-0.4, cmap=cmap)\n",
    "        ax1.set_title('Argmax. Iteration # ' + str(i))\n",
    "        probability_graph = ax2.imshow(probabilities_np.squeeze()[:, :, 0])\n",
    "        ax2.set_title('Probability of the Class. Iteration # ' + str(i))\n",
    "\n",
    "        plt.colorbar(probability_graph)\n",
    "        plt.show()\n",
    "\n",
    "        #print(\"Current Loss: \" +  str(loss))\n",
    "\n",
    "    feed_dict_to_use[is_training_placeholder] = False\n",
    "\n",
    "    final_predictions, final_probabilities, final_loss = sess.run([pred,\n",
    "                                                                   probabilities,\n",
    "                                                                   cross_entropy_sum],\n",
    "                                                         feed_dict=feed_dict_to_use)\n",
    "\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "    ax1.imshow(np.uint8(final_predictions.squeeze() != 1),\n",
    "               vmax=1.5,\n",
    "               vmin=-0.4,\n",
    "               cmap=cmap)\n",
    "\n",
    "    ax1.set_title('Final Argmax')\n",
    "\n",
    "\n",
    "    probability_graph = ax2.imshow(final_probabilities.squeeze()[:, :, 0])\n",
    "    ax2.set_title('Final Probability of the Class')\n",
    "    plt.colorbar(probability_graph)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Loss: \" +  str(final_loss))\n",
    "\n",
    "summary_string_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
